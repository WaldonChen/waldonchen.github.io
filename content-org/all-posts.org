#+TITLE: All Posts
#+language: zh-CN
#+hugo_base_dir: ../

#+bibliography: biblio.bib
#+cite_export: csl ieee.csl

* DONE 深度学习编译器                                      :@深度学习编译器:tvm:
CLOSED: [2022-07-07 四 11:03]
:PROPERTIES:
:EXPORT_FILE_NAME: deep-learning-compiler
:EXPORT_OPTIONS: ^:nil H:2 num:3
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :tocNum false
:END:

本文整理深度学习编译器相关的论文和来自知乎等网站的相关材料[cite:@chen-2022-tvm-related]。

** 深度学习编译器相关论文

*** Survey

- The Deep Learning Compiler: A Comprehensive Survey

  #+begin_quote
  DL编译器的survey，总结了DL编译器的设计框架
  #+end_quote

- An In-depth Comparison of Compilers for Deep Neural Networks on Hardware

  #+begin_quote
  比较了Halide, XLA, TVM, TC等几种编译器的性能
  #+end_quote

*** TVM系列

**** OSDI'18 - TVM: An Automated End-to-End Optimizing Compiler for Deep Learning

文章[cite:@chen-2018-tvm-automa]完整介绍了 TVM 的设计的背景，目标，技术难点和解
决方案。该文章确定了整个 TVM 的技术架构，包括硬件无关的图级别的优化，硬件相关的
算子优化，基于代价模型的搜索寻优等等。快速了解此文可以阅读已有的一些[[https://zhuanlan.zhihu.com/p/498115380][论文阅读笔记]]。

**** IWMLPL'18 - Relay - A New IR for Machine Learning Frameworks

Relay[cite:@roesch-2018-relay]是TVM 新的中间表示形式，不同框架的模型先转化成
Relay，然后在Relay 上来做图优化。[[https://www.zhihu.com/question/331611341/answer/875630325][如何评价TVM的新IR（Relay）？]]

**** Relay: A High-Level Compiler for Deep Learning[cite:@roesch-2019-relay]

#+begin_quote
TVM的第二代high-level IR，类似于编程语言，设计了语法规
则，引入了let-binding机制。DL背景的开发者可以使用data flow graph来定义计算图，
PL(Program Language)背景的研究人员可以使用let binding来定义计算图。Let binding机
制通过compute scope解决了AST的二义性问题。
#+end_quote

[[https://zhuanlan.zhihu.com/p/462831936][Relay: A High-Level Compiler for Deep Learning 论文翻译]]

*** Auto-tuning相关工作

**** NIPS'18 - Learning to Optimize Tensor Programs

文章[cite:@chen-2018-learni]发表于 2018 年的NIPS，详细介绍了autotvm 自动寻优方案。
文章是对论文[cite:@chen-2018-tvm-automa]的第5节的一个更加详细的扩充。主要观点在
OSDI2018 的论文里都有描述，论文 1 没有涉及的的内容是引入了 transfer learning 的
应用。

[[https://zhuanlan.zhihu.com/p/457573510][Learning to Optimize Tensor Programs解读]]

**** OSDI'20 - Ansor: Generating High-Performance Tensor Programs for Deep Learning

Ansor 论文[cite:@zheng-2020-ansor]发表于2020 年 OSDI ， 其核心目标是生成高效的程
序，具体包括两个部分(1)如何扩大搜索空间(2)如何提高搜索的性能和效率。

为了扩大搜索空间，改进了TVM 基于模板的搜索空间定义，通过层次化搜索的方案，解耦
high-level 结构和low-level 细节。

为了提升搜索的性能和效率，ansor 改进了TVM 的搜索策略，将搜索算法从模拟退火算法修
改成了遗传算法(原文是进化搜索 evolutionary search，国内多翻译为遗传算法)，从而能
够有效的跳出局部最优。

#+begin_quote
把schedule分成sketch和annotation两层，sketch相当于TVM的schedule template，Ansor
可以先搜索出sketch，再搜索annotation。
#+end_quote

**** FlexTensor: An Automatic Schedule Exploration and Optimization Framework for Tensor Computation on Heterogeneous System

FlexTensor [cite:@zheng-2020-flext]

**** CHAMELEON: ADAPTIVE CODE OPTIMIZATION FOR EXPEDITED DEEP NEURAL NETWORK COMPILATION

*** Polyhedral

- PLDI'21 - AKG: Automatic Kernel Generation for Neural Processing Units using Polyhedral Transformations [cite:@zhao-2021-akg]

 [[https://zhuanlan.zhihu.com/p/384191216][PLDI 2021论文分析(一)：AKG-NPU上算子自动生成技术探索]]

- MICRO'20 - Optimizing the Memory Hierarchy by Compositing Automatic Transformations on Computations and Data [cite:@zhao-2020-optim-memor]

 [[https://zhuanlan.zhihu.com/p/333394142][53年来国内唯三，MindSpore加速昇腾芯片论文获国际顶会MICRO最佳论文提名]]


- [[https://mp.weixin.qq.com/s/QEooKxP1sm5O90AUiqKQEQ][一.Poly基本原理及卷积分析示例]]
- [[https://mp.weixin.qq.com/s/NRtud1UImE5ArZ2zQWFRyg][二. Poly在深度学习领域中发挥的作用]]
- [[https://mp.weixin.qq.com/s/bLBIrJb82IsnyoXSEr2xtw][三. AI芯片上利用Poly进行软硬件优化的一些问题]]

#+begin_quote
上面面三篇公众号文章介绍Poly的一些基本原理和在DL领域中的应用，作者是要术甲杰，是
Poly研究领域的博士
#+end_quote

- [[https://zhuanlan.zhihu.com/p/199683290?utm_source=wechat_session&utm_medium=social&utm_oi=848584440992141312][Polyhedral编译调度算法(1)------Pluto算法]]

#+begin_quote
同样是要术甲杰写的介绍Pluto算法的文章
#+end_quote

*** Others

- Tensorflow XLA/JAX
- MLIR
- GLOW @facebook
- Halide
- Tensor comprehension

- [[https://zhuanlan.zhihu.com/p/141256429?utm_source=wechat_session&utm_medium=social&utm_oi=837261071604645888&wechatShare=1&s_r=0][MLIR文章视频汇总]]

- [[https://github.com/MondayYuan/DLCompilerResource/blob/master/pdf][Multi-Level Tactics: Abstraction Raising in Multi-Level IR]]

- [[https://github.com/MondayYuan/DLCompilerResource/blob/master/pdf/FusionStitching.pdf][FUSIONSTITCHING: DEEP FUSION AND CODE GENERATION FOR TENSORFLOW COMPUTATIONS ON GPUS]]

#+begin_quote
用shared memory来实现更激进的operator fusion策略
#+end_quote

- [[https://github.com/MondayYuan/DLCompilerResource/blob/master/pdf/AutoDiffinML.pdf][Automatic differentiation in ML: Where we are and where we should be going]]
- [[https://github.com/MondayYuan/DLCompilerResource/blob/master/pdf/AutoDiffSurvey.pdf][Automatic Differentiation in Machine Learning: a Survey]]

#+begin_quote
两篇关于自动微分的survey
#+end_quote

- [[https://github.com/MondayYuan/DLCompilerResource/blob/master/pdf/IREE.pdf][IREE: MLIR-based End-to-End ML Tooling]]

#+begin_quote
schedule和execution阶段进行联合优化
#+end_quote

- [[https://zhuanlan.zhihu.com/p/163717035][AI编译优化--总纲]]
- [[https://zhuanlan.zhihu.com/p/163857096][访存密集算子优化]]
- [[https://zhuanlan.zhihu.com/p/174817186][计算密集算子优化]]
- [[https://zhuanlan.zhihu.com/p/194353051][AI编译优化--业务实践]]

#+begin_quote
阿里杨军的系列文章
#+end_quote

- [[https://github.com/MondayYuan/DLCompilerResource/blob/master/pdf/swTVM.pdf][swTVM: Exploring the Automated Compilation for Deep Learning on Sunway Architecture]]

#+begin_quote
用TVM在神威超算上生成算子
#+end_quote

- [[https://github.com/MondayYuan/DLCompilerResource/blob/master/pdf/TFGraphOptimizationsStanford.pdf][TensorFlow Graph Optimizations]]

#+begin_quote
TensorFow中的图优化
#+end_quote

** 其它相关文档

*** [[https://www.zhihu.com/people/jin-xue-feng][AI编译器@金雪峰]]

- [[https://zhuanlan.zhihu.com/p/508345356][AI编译器的概览、挑战和实践]]
- [[https://www.zhihu.com/question/396105855/answer/1868408680][针对神经网络的编译器和传统编译器的区别和联系是什么？]]

  [[https://zhuanlan.zhihu.com/p/342865488][深度学习编译器系列之嵌入在 AI 框架中的深度学习编译器]]

*** [[https://www.zhihu.com/column/c_1388629436179378176][漫游深度学习编译器@知乎]]

- [[https://zhuanlan.zhihu.com/p/381324332][TVM系列「一」TVM概览]]
- [[https://zhuanlan.zhihu.com/p/381330616][TVM系列「二」TVM学习资源]]
- [[https://zhuanlan.zhihu.com/p/381331888][TVM系列「三」TVM官方文档的结构]]
- [[https://zhuanlan.zhihu.com/p/381333188][TVM系列「四」TVM的使用：compute+schedule双剑合璧]]
- [[https://zhuanlan.zhihu.com/p/381691430][TVM系列「五」TVM整体架构及其代码生成]]
- [[https://zhuanlan.zhihu.com/p/390087648][TVM系列「六」Relay IR与Relay Pass]]
- [[https://zhuanlan.zhihu.com/p/392015642][TVM系列「七」AutoTVM（AutoTune）]]
- [[https://zhuanlan.zhihu.com/p/394765523][TVM系列「八」AutoScheduler「Ansor」]]

*** [[https://www.zhihu.com/column/c_1254058636869603328][TVM代码走读【@知乎专栏】]]

- [[https://zhuanlan.zhihu.com/p/145676823][TVM代码走读（一） ONNX前端]]
- [[https://zhuanlan.zhihu.com/p/149386093][TVM代码走读（二） 算子实现]]
- [[https://zhuanlan.zhihu.com/p/149988448][TVM代码走读（三） 图优化1--初识PASS]]
- [[https://zhuanlan.zhihu.com/p/151351056][TVM代码走读（四） 图优化2--TVM RELAY树结构]]
- [[https://zhuanlan.zhihu.com/p/151815380][TVM代码走读（五） 图优化3-- Constant Folding]]
- [[https://zhuanlan.zhihu.com/p/153098112][TVM代码走读（六） 图优化4-- Fuse ops]]
- [[https://zhuanlan.zhihu.com/p/161195053][TVM代码走读（七） 模型编译1--调用链]]
- [[https://zhuanlan.zhihu.com/p/165236267][TVM代码走读（八） 模型编译2--编译优化]]
- [[https://zhuanlan.zhihu.com/p/166551011][TVM代码走读（九） 计算和调度]]
- [[https://zhuanlan.zhihu.com/p/181399538][TVM代码走读（十） AutoTvm]]
- [[https://zhuanlan.zhihu.com/p/208594323][TVM代码走读（十一） te::Stage]]
- [[https://zhuanlan.zhihu.com/p/202938038][TVM代码走读（十二） lower--phase 0]]
- [[https://zhuanlan.zhihu.com/p/245453441][TVM代码走读（十三） arith::analyzer]]
- [[https://zhuanlan.zhihu.com/p/257128533][TVM代码走读（十四） lower--ScheduleOps]]
- [[https://zhuanlan.zhihu.com/p/263581574][TVM代码走读（十五） lower--relay func到lower的流程梳理]]
- [[https://zhuanlan.zhihu.com/p/267986568][TVM代码走读（十六） lower--phase0后续流程]]
- [[https://zhuanlan.zhihu.com/p/275708458][TVM代码走读（十七） lower--phase 1]]
- [[https://zhuanlan.zhihu.com/p/307689224][TVM代码走读（十八） lower--LoopPartition(phase 2)]]

*** [[https://www.zhihu.com/people/liu-jia-qiang-18-27][深度学习编译器@柳嘉强]]

- [[https://zhuanlan.zhihu.com/p/307750772][1. 深度学习编译器 - 引言]]
- [[https://zhuanlan.zhihu.com/p/309793908][2. 深度学习编译器 - 前端]]
- [[https://zhuanlan.zhihu.com/p/338299844][3. 深度学习编译器 - 图表示]]
- [[https://zhuanlan.zhihu.com/p/344752857][4. 深度学习编译器 - 自动微分]]
- [[https://zhuanlan.zhihu.com/p/412217136][5. 深度学习编译器 - 图优化]]
- [[https://zhuanlan.zhihu.com/p/436025551][5. 深度学习编译器 - 图优化（续)]]
- [[https://zhuanlan.zhihu.com/p/469972467][6. 深度学习编译器 - 低精度计算之量化]]
- [[https://zhuanlan.zhihu.com/p/511043383][7. 深度学习编译器 - 算子的高效实现]]

*** [[https://github.com/BBuf/tvm_learn][从零开始学习深度学习编译器]]

- [[https://mp.weixin.qq.com/s/sZLWjYebbHjCgQ6XAZCiOw][一，深度学习编译器及TVM 介绍]]
- [[https://mp.weixin.qq.com/s/fPpqKL3uaaJ5QlNS79DZ5Q][二，TVM中的scheduler]]
- [[https://mp.weixin.qq.com/s/KFxd3zf76EP3DFcCAPZjvQ][三，基于ONNX模型结构了解TVM的前端]]
- [[https://mp.weixin.qq.com/s/1YlTSUArDIzY-9zeUAIfhQ][四，解析TVM算子]]
- [[https://mp.weixin.qq.com/s/5JAWE9RTTXwDJR5HqlsCzA][五，TVM Relay以及Pass简介]]
- [[https://mp.weixin.qq.com/s/CZzC5klWoFftUlOKkpvEZg][六，TVM的编译流程详解]]
- [[https://mp.weixin.qq.com/s/IMm1nurpoESFRLxHcEYxcQ][七，万字长文入门TVM Pass]]
- [[https://mp.weixin.qq.com/s/QphPwnRE5uANJk2qiqlI6w][八，TVM的算符融合以及如何使用TVM Pass Infra自定义Pass]]
- [[https://mp.weixin.qq.com/s/n7-ZTzCwFOvHrrzg4gFXQQ][九，TVM的CodeGen流程]]
- [[https://mp.weixin.qq.com/s/9nnrXhzP_gqFEPuIMdEE5w][十，TVM的整体把握]]
- [[https://mp.weixin.qq.com/s/4pD00N9HnPiIYUOGSnSuIw][十一，初识MLIR]]
- [[https://mp.weixin.qq.com/s/jMHesvKmAUU5dYH0WznulA][十二，MLIR Toy Tutorials学习笔记一]]
- [[https://mp.weixin.qq.com/s/3N9DK7aQtjoLgs-s0lP-jg][十三，如何在MLIR里面写Pass？]]
- [[https://mp.weixin.qq.com/s/3hAf7zxEKwRvnVAKhziTmA][十四，MLIR Toy Tutorials学习笔记之部分Lowering]]
- [[https://mp.weixin.qq.com/s/ve2l3luRzIeDwG4PHjhDlQ][十五，MLIR Toy Tutorials学习笔记之Lowering到LLVM IR]]
- [[https://mp.weixin.qq.com/s/SFHWUm63BqsD9SWwuW83mA][十六，MLIR ODS要点总结上篇]]
- [[https://mp.weixin.qq.com/s/TsaMULNUXIVlUPnVs2WexA][十七，MLIR ODS要点总结下篇]]
- [[https://mp.weixin.qq.com/s/yD-b75p1An4YTpfoIgB8mQ][十八，MLIR中的Interfaces]]
- [[https://mp.weixin.qq.com/s/qmFpGtH0oB_ml0LQGPUqPA][十九，MLIR的Pass机制实践]]
- [[https://mp.weixin.qq.com/s/7QwJvTZ9Z2KbUwxqvQHC2g][二十，MLIR的Pattern Rewrite机制]]
- [[https://mp.weixin.qq.com/s/Kt4xDLo-NRui8Whl0DqcSA][番外一，Data Flow和Control Flow]]
- [[https://mp.weixin.qq.com/s/7Wvv4VOPdj6N_CEg8bJFXw][番外二，在Jetson Nano上玩TVM]]

**** 其它

- [[https://mp.weixin.qq.com/s/ohWy5yBrsKpzApfjQLXWJg][白杨：TVM源语-Compute篇]]
- [[https://mp.weixin.qq.com/s/79lzlCHAxQEE0EQcxL07XQ][MLSys 15-884: Course Introduction]]
- [[https://zhuanlan.zhihu.com/p/533807811][OSDI 2021 PET 论文解读（代码生成相关工作）]]
- [[https://mp.weixin.qq.com/s/uE5VhU_s3NgndPk2X6zbAA][Buddy-MLIR 项目详解（入门 MLIR 极佳选择）]]
- [[https://mp.weixin.qq.com/s/mwIc9DZo4r7YgYsPus-2tA][【社区实践】为 TVM 新增 OneFlow 前端]]
- [[https://mp.weixin.qq.com/s/SLzMKYugrkhQifqahfdVNw][MLIR：摩尔定律终结的编译器基础结构 论文解读]]
- [[https://mp.weixin.qq.com/s/d8v9Q3EAkv8TknP5Hh7N7A][【TVM 三代优化巡礼】在X86上将普通的矩阵乘法算子提速90倍]]
- [[https://mp.weixin.qq.com/s/gbpqYwPbtHp1RIYPD_ZlCg][【论文解读】基于MLIR生成矩阵乘法的高性能GPU代码，性能持平cuBLAS]]
- [[https://mp.weixin.qq.com/s/JENCa_GNGPHhOspGb79ugA][【用沐神的方法阅读PyTorch FX论文】]]
- [[https://mp.weixin.qq.com/s/eUIm4QZbKU69B9_h3f109A][【以OneFlow为例探索MLIR的实际开发流程】]]
- [[https://mp.weixin.qq.com/s/goAtJKe6p0e3pbp5vcQWfA][可以让深度学习编译器来指导算子优化吗]]
- [[https://mp.weixin.qq.com/s/OJCHzh4opNN2Mnomz_6L9Q][Ansor论文阅读笔记&&论文翻译]]

** References                                                          :ignore:

#+print_bibliography:

# ** Bibliography                                                      :noexport:

# [[bibliography:biblio.bib]]
